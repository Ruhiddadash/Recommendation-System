# -*- coding: utf-8 -*-
"""CollabFiltering Vol2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QYKsVbfOQ9--jmmXuneCLpqmav4ALIE4
"""

!pip install numpy pandas

"""
============================================
COLLABORATIVE FILTERING ALGORITHMS
MovieLens 25M Dataset (Up-to-date movies)
============================================
"""

import numpy as np
import pandas as pd
from urllib.request import urlretrieve
import os
import zipfile
from datetime import datetime

class CollaborativeFiltering:
    def __init__(self):
        self.ratings_df = None
        self.movies_df = None
        self.user_item_matrix = None
        self.item_user_matrix = None
        self.n_users = 0
        self.n_items = 0
        self.user_id_map = {}  # Map original user IDs to matrix indices
        self.item_id_map = {}  # Map original movie IDs to matrix indices
        self.reverse_item_map = {}  # Map matrix indices back to movie IDs

    # ============================================
    # DATA LOADING FROM MOVIELENS 25M
    # ============================================

    def load_movielens_data(self, dataset='25m', sample_users=5000):
        """
        Load MovieLens dataset with up-to-date movies

        Parameters:
        -----------
        dataset : str
            Dataset size: '1m', '10m', or '25m' (default: '25m')
            - 1m: 1 million ratings, 3,706 movies (2003)
            - 10m: 10 million ratings, 10,681 movies (2009)
            - 25m: 25 million ratings, 62,423 movies (2019) - RECOMMENDED
        sample_users : int
            Number of users to sample for faster processing (default: 5000)
            Set to None to use all users (slower but more accurate)
        """
        data_paths = {
            '1m': ('ml-1m', 'https://files.grouplens.org/datasets/movielens/ml-1m.zip'),
            '10m': ('ml-10m', 'https://files.grouplens.org/datasets/movielens/ml-10m.zip'),
            '25m': ('ml-25m', 'https://files.grouplens.org/datasets/movielens/ml-25m.zip')
        }

        if dataset not in data_paths:
            raise ValueError(f"Dataset must be one of: {list(data_paths.keys())}")

        data_path, url = data_paths[dataset]

        # Download dataset if not exists
        if not os.path.exists(data_path):
            print(f"Downloading MovieLens {dataset.upper()} dataset...")
            print(f"This may take a while (dataset size: ~250MB for 25m)...")
            zip_file = f'{data_path}.zip'
            urlretrieve(url, zip_file)

            print("Extracting dataset...")
            with zipfile.ZipFile(zip_file, 'r') as zip_ref:
                zip_ref.extractall('.')
            os.remove(zip_file)
            print("Dataset downloaded and extracted!")

        # Load ratings based on dataset format
        print(f"Loading ratings from {dataset.upper()}...")

        if dataset == '1m':
            # Format: UserID::MovieID::Rating::Timestamp
            self.ratings_df = pd.read_csv(
                os.path.join(data_path, 'ratings.dat'),
                sep='::',
                engine='python',
                names=['user_id', 'movie_id', 'rating', 'timestamp']
            )
            # Load movies
            self.movies_df = pd.read_csv(
                os.path.join(data_path, 'movies.dat'),
                sep='::',
                engine='python',
                encoding='latin-1',
                names=['movie_id', 'title', 'genres']
            )
        else:
            # Format for 10m and 25m: userId,movieId,rating,timestamp
            self.ratings_df = pd.read_csv(
                os.path.join(data_path, 'ratings.csv')
            )
            # Rename columns to match our convention
            self.ratings_df.columns = ['user_id', 'movie_id', 'rating', 'timestamp']

            # Load movies
            self.movies_df = pd.read_csv(
                os.path.join(data_path, 'movies.csv')
            )
            self.movies_df.columns = ['movie_id', 'title', 'genres']

        # Extract year from title
        self.movies_df['year'] = self.movies_df['title'].str.extract(r'\((\d{4})\)')

        # Sample users if specified (for faster processing)
        if sample_users is not None and sample_users < self.ratings_df['user_id'].nunique():
            print(f"Sampling {sample_users} most active users for faster processing...")
            # Get top N most active users
            user_counts = self.ratings_df['user_id'].value_counts()
            top_users = user_counts.head(sample_users).index
            self.ratings_df = self.ratings_df[self.ratings_df['user_id'].isin(top_users)]

        # Filter movies that have ratings
        rated_movies = self.ratings_df['movie_id'].unique()
        self.movies_df = self.movies_df[self.movies_df['movie_id'].isin(rated_movies)]

        print(f"\nDataset Statistics:")
        print(f"Total Ratings: {len(self.ratings_df):,}")
        print(f"Unique Users: {self.ratings_df['user_id'].nunique():,}")
        print(f"Unique Movies: {self.ratings_df['movie_id'].nunique():,}")

        # Show some recent movies
        recent_movies = self.movies_df.sort_values('year', ascending=False).head(10)
        print(f"\nSample of recent movies in dataset:")
        for _, movie in recent_movies.iterrows():
            print(f"  - {movie['title']} | {movie['genres']}")

        # Build matrices
        self.build_matrices()

        return self

    # ============================================
    # BUILD USER-ITEM AND ITEM-USER MATRICES
    # ============================================

    def build_matrices(self):
        """
        Build user-item rating matrix with ID mapping
        """
        # Create mappings for sparse matrix
        unique_users = sorted(self.ratings_df['user_id'].unique())
        unique_items = sorted(self.ratings_df['movie_id'].unique())

        self.user_id_map = {uid: idx for idx, uid in enumerate(unique_users)}
        self.item_id_map = {iid: idx for idx, iid in enumerate(unique_items)}
        self.reverse_item_map = {idx: iid for iid, idx in self.item_id_map.items()}

        self.n_users = len(unique_users)
        self.n_items = len(unique_items)

        print(f"\nBuilding matrices for {self.n_users} users and {self.n_items} items...")

        # Create user-item matrix
        self.user_item_matrix = np.zeros((self.n_users, self.n_items))

        for row in self.ratings_df.itertuples():
            user_idx = self.user_id_map[row.user_id]
            item_idx = self.item_id_map[row.movie_id]
            self.user_item_matrix[user_idx, item_idx] = row.rating

        # Create item-user matrix (transpose)
        self.item_user_matrix = self.user_item_matrix.T

        sparsity = (1 - len(self.ratings_df) / (self.n_users * self.n_items)) * 100

        print(f"Matrices built successfully!")
        print(f"Matrix shape: {self.user_item_matrix.shape}")
        print(f"Sparsity: {sparsity:.2f}%")

    # ============================================
    # SIMILARITY METRICS (Optimized for large data)
    # ============================================

    def pearson_correlation(self, vec1, vec2):
        """
        Calculate Pearson Correlation between two rating vectors
        Optimized for sparse data
        """
        # Find indices where both vectors have ratings
        mask = (vec1 > 0) & (vec2 > 0)

        if np.sum(mask) < 2:
            return 0.0

        # Get common ratings
        v1 = vec1[mask]
        v2 = vec2[mask]

        # Calculate correlation
        mean1 = np.mean(v1)
        mean2 = np.mean(v2)

        numerator = np.sum((v1 - mean1) * (v2 - mean2))
        denominator = np.sqrt(np.sum((v1 - mean1)**2) * np.sum((v2 - mean2)**2))

        if denominator == 0:
            return 0.0

        return numerator / denominator

    def cosine_similarity(self, vec1, vec2):
        """
        Calculate Cosine Similarity between two rating vectors
        Optimized for sparse data
        """
        # Find indices where both vectors have ratings
        mask = (vec1 > 0) & (vec2 > 0)

        if np.sum(mask) == 0:
            return 0.0

        # Calculate dot product and norms
        dot_product = np.sum(vec1[mask] * vec2[mask])
        norm1 = np.sqrt(np.sum(vec1[vec1 > 0]**2))
        norm2 = np.sqrt(np.sum(vec2[vec2 > 0]**2))

        if norm1 == 0 or norm2 == 0:
            return 0.0

        return dot_product / (norm1 * norm2)

    # ============================================
    # USER-BASED COLLABORATIVE FILTERING
    # ============================================

    def user_based_cf(self, user_id, k=30, top_n=10):
        """
        Generate recommendations using User-Based CF

        Parameters:
        -----------
        user_id : int
            Original user ID from dataset
        k : int
            Number of similar users (default: 30)
        top_n : int
            Number of recommendations (default: 10)
        """
        if user_id not in self.user_id_map:
            raise ValueError(f"User ID {user_id} not found in dataset")

        user_idx = self.user_id_map[user_id]
        target_user_ratings = self.user_item_matrix[user_idx]

        print(f"Finding similar users for User {user_id}...")
        similarities = []

        # Calculate similarity with other users (sample if too many)
        sample_size = min(1000, self.n_users - 1)
        user_indices = np.random.choice(
            [i for i in range(self.n_users) if i != user_idx],
            size=sample_size,
            replace=False
        )

        for other_idx in user_indices:
            similarity = self.pearson_correlation(
                target_user_ratings,
                self.user_item_matrix[other_idx]
            )

            if similarity > 0:
                similarities.append({
                    'user_idx': other_idx,
                    'similarity': similarity
                })

        # Sort by similarity and get top K
        similarities = sorted(similarities, key=lambda x: x['similarity'], reverse=True)[:k]
        print(f"Found {len(similarities)} similar users")

        # Predict ratings for unrated items
        predictions = []
        unrated_items = np.where(target_user_ratings == 0)[0]

        for item_idx in unrated_items:
            numerator = 0
            denominator = 0

            for neighbor in similarities:
                rating = self.user_item_matrix[neighbor['user_idx'], item_idx]
                if rating > 0:
                    numerator += neighbor['similarity'] * rating
                    denominator += abs(neighbor['similarity'])

            if denominator > 0:
                predicted_rating = numerator / denominator
                movie_id = self.reverse_item_map[item_idx]
                movie_info = self.movies_df[self.movies_df['movie_id'] == movie_id].iloc[0]

                predictions.append({
                    'movie_id': movie_id,
                    'title': movie_info['title'],
                    'genres': movie_info['genres'],
                    'year': movie_info['year'],
                    'predicted_rating': predicted_rating
                })

        # Sort and return top N
        predictions = sorted(predictions, key=lambda x: x['predicted_rating'], reverse=True)
        return predictions[:top_n]

    # ============================================
    # ITEM-BASED COLLABORATIVE FILTERING
    # ============================================

    def item_based_cf(self, user_id, k=30, top_n=10):
        """
        Generate recommendations using Item-Based CF

        Parameters:
        -----------
        user_id : int
            Original user ID from dataset
        k : int
            Number of similar items (default: 30)
        top_n : int
            Number of recommendations (default: 10)
        """
        if user_id not in self.user_id_map:
            raise ValueError(f"User ID {user_id} not found in dataset")

        user_idx = self.user_id_map[user_id]
        target_user_ratings = self.user_item_matrix[user_idx]

        print(f"Generating item-based recommendations for User {user_id}...")

        # Get items user has rated
        rated_items = np.where(target_user_ratings > 0)[0]
        unrated_items = np.where(target_user_ratings == 0)[0]

        predictions = []

        # Sample unrated items if too many
        if len(unrated_items) > 1000:
            unrated_items = np.random.choice(unrated_items, size=1000, replace=False)

        for item_idx in unrated_items:
            similarities = []

            # Calculate similarity with rated items
            for rated_idx in rated_items:
                similarity = self.cosine_similarity(
                    self.item_user_matrix[item_idx],
                    self.item_user_matrix[rated_idx]
                )

                if similarity > 0:
                    similarities.append({
                        'item_idx': rated_idx,
                        'similarity': similarity,
                        'rating': target_user_ratings[rated_idx]
                    })

            # Sort and get top K
            similarities = sorted(similarities, key=lambda x: x['similarity'], reverse=True)[:k]

            # Calculate predicted rating
            numerator = 0
            denominator = 0

            for sim_item in similarities:
                numerator += sim_item['similarity'] * sim_item['rating']
                denominator += abs(sim_item['similarity'])

            if denominator > 0:
                predicted_rating = numerator / denominator
                movie_id = self.reverse_item_map[item_idx]
                movie_info = self.movies_df[self.movies_df['movie_id'] == movie_id].iloc[0]

                predictions.append({
                    'movie_id': movie_id,
                    'title': movie_info['title'],
                    'genres': movie_info['genres'],
                    'year': movie_info['year'],
                    'predicted_rating': predicted_rating
                })

        # Sort and return top N
        predictions = sorted(predictions, key=lambda x: x['predicted_rating'], reverse=True)
        return predictions[:top_n]

    # ============================================
    # UTILITY METHODS
    # ============================================

    def get_user_ratings(self, user_id, top_n=10):
        """Get a user's rated movies"""
        user_ratings = self.ratings_df[self.ratings_df['user_id'] == user_id]
        user_ratings = user_ratings.merge(self.movies_df, on='movie_id')
        return user_ratings[['title', 'rating', 'genres', 'year']].sort_values('rating', ascending=False).head(top_n)

    def search_movies(self, query, limit=10):
        """Search movies by title"""
        results = self.movies_df[self.movies_df['title'].str.contains(query, case=False, na=False)]
        return results[['movie_id', 'title', 'genres', 'year']].head(limit)

    def get_random_user(self):
        """Get a random active user ID"""
        return np.random.choice(list(self.user_id_map.keys()))


# ============================================
# USAGE EXAMPLE
# ============================================

def main():
    print("=" * 80)
    print("MovieLens Collaborative Filtering with Up-to-Date Movies")
    print("=" * 80)

    # Initialize CF system
    cf = CollaborativeFiltering()

    # Load MovieLens 25M dataset (up-to-date movies)
    # Use sample_users for faster processing, or None for full dataset
    cf.load_movielens_data(dataset='25m', sample_users=5000)

    # Get a random user
    user_id = cf.get_random_user()

    print(f"\n{'=' * 80}")
    print(f"Recommendations for User {user_id}")
    print("=" * 80)

    # Get user's ratings
    user_ratings = cf.get_user_ratings(user_id, top_n=10)
    print(f"\nUser's Top Rated Movies:")
    print("-" * 80)
    for idx, row in user_ratings.iterrows():
        print(f"  {row['rating']}â˜… - {row['title']}")
        print(f"      Genres: {row['genres']}")

    # User-Based CF
    print(f"\n{'=' * 80}")
    print("User-Based Collaborative Filtering Recommendations")
    print("=" * 80)
    user_recs = cf.user_based_cf(user_id, k=30, top_n=10)
    for idx, rec in enumerate(user_recs, 1):
        print(f"{idx}. {rec['title']}")
        print(f"   Predicted: {rec['predicted_rating']:.2f}/5.0")
        print(f"   Genres: {rec['genres']}\n")

    # Item-Based CF
    print(f"{'=' * 80}")
    print("Item-Based Collaborative Filtering Recommendations")
    print("=" * 80)
    item_recs = cf.item_based_cf(user_id, k=30, top_n=10)
    for idx, rec in enumerate(item_recs, 1):
        print(f"{idx}. {rec['title']}")
        print(f"   Predicted: {rec['predicted_rating']:.2f}/5.0")
        print(f"   Genres: {rec['genres']}\n")

    # Search example
    print(f"{'=' * 80}")
    print("Movie Search Example: 'Avengers'")
    print("=" * 80)
    search_results = cf.search_movies('Fight Club')
    for _, movie in search_results.iterrows():
        print(f"  - {movie['title']} | {movie['genres']}")


if __name__ == "__main__":
    main()