# -*- coding: utf-8 -*-
"""Contentbase.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sTqvcKcCDYruTCyRlnXQaBAS4LEJsz8P
"""

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import normalize
from sklearn.neighbors import NearestNeighbors
from scipy.sparse import csr_matrix
from sklearn.model_selection import train_test_split

# Load datasets
movies = pd.read_csv('movies.csv')
ratings = pd.read_csv('ratings.csv')
try:
    tags = pd.read_csv('tags.csv')
except:
    tags = pd.DataFrame(columns=['userId','movieId','tag','timestamp'])

# Prepare movie metadata
movies['title'] = movies['title'].astype(str)
movies['genres'] = movies['genres'].fillna('').astype(str)
tags = tags.dropna(subset=['tag'])
tags['tag'] = tags['tag'].astype(str)
tag_agg = tags.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).reset_index()
movies = movies.merge(tag_agg, how='left', on='movieId')
movies['tag'] = movies['tag'].fillna('')
movies['metadata'] = movies['title'] + ' ' + movies['genres'].str.replace('|',' ') + ' ' + movies['tag']

# TF-IDF for content
tfidf = TfidfVectorizer(stop_words='english', max_features=10000)
movie_tfidf = tfidf.fit_transform(movies['metadata'])

# Build mappings
movie_id_to_idx = {mid: idx for idx, mid in enumerate(movies['movieId'].values)}
idx_to_movie_id = {v:k for k,v in movie_id_to_idx.items()}
title_to_indices = {}
for idx, t in enumerate(movies['title'].values):
    title_to_indices.setdefault(t.lower(), []).append(idx)

# Train-test split for evaluation
train_ratings, test_ratings = train_test_split(ratings, test_size=0.2, random_state=42)

# Build train matrix
rows, cols, data_vals = [], [], []
for _, r in train_ratings.iterrows():
    mid = r['movieId']
    uid = int(r['userId'])
    if mid in movie_id_to_idx:
        rows.append(uid)
        cols.append(movie_id_to_idx[mid])
        data_vals.append(r['rating'])
n_users = ratings['userId'].max()+1
n_movies = len(movies)
train_matrix = csr_matrix((data_vals, (rows, cols)), shape=(n_users, n_movies))

# Collaborative filtering: SVD
n_components = 50
svd = TruncatedSVD(n_components=n_components, random_state=42)
svd_movie_factors = svd.fit_transform(train_matrix.T)
svd_user_factors = train_matrix.dot(svd_movie_factors)

# Combine content + collaborative embeddings
movie_tfidf_dense = movie_tfidf.toarray()
movie_tfidf_norm = normalize(movie_tfidf_dense, axis=1)
svd_movie_norm = normalize(svd_movie_factors, axis=1)
movie_embeddings = np.hstack([movie_tfidf_norm, svd_movie_norm])
movie_embeddings = normalize(movie_embeddings, axis=1)

# Nearest Neighbors model
nn_model = NearestNeighbors(n_neighbors=20, metric='cosine', algorithm='brute')
nn_model.fit(movie_embeddings)

# Helper functions
def _find_title_index(query):
    q = query.strip().lower()
    if q in title_to_indices:
        return title_to_indices[q][0]
    candidates = movies[movies['title'].str.lower().str.contains(q, na=False)]
    if not candidates.empty:
        return int(candidates.index[0])
    raise KeyError(query)

def get_similar_movies_by_title(title, n=5):
    idx = _find_title_index(title)
    emb = movie_embeddings[idx].reshape(1, -1)
    dists, inds = nn_model.kneighbors(emb, n_neighbors=n+1)
    inds = [i for i in inds.flatten() if i != idx][:n]
    return movies['title'].iloc[inds].tolist()

def get_similar_movies_by_titles(titles, n=5):
    idxs = []
    for t in titles:
        try:
            idxs.append(_find_title_index(t))
        except:
            pass
    if not idxs:
        return []
    emb = movie_embeddings[idxs].mean(axis=0).reshape(1, -1)
    dists, inds = nn_model.kneighbors(emb, n_neighbors=n+len(idxs)+1)
    inds = [i for i in inds.flatten() if i not in idxs][:n]
    return movies['title'].iloc[inds].tolist()

def recommend_for_user(userId, n=10):
    if userId > train_matrix.shape[0]-1:
        return []
    user_vector = svd_user_factors[userId]
    movie_scores_cf = movie_embeddings[:, -n_components:].dot(user_vector)
    user_rated = train_matrix[userId].nonzero()[1]
    mask = np.ones(len(movie_scores_cf), dtype=bool)
    mask[user_rated] = False
    candidate_idx = np.where(mask)[0]
    top_idx = candidate_idx[np.argsort(-movie_scores_cf[candidate_idx])][:n]
    return movies['title'].iloc[top_idx].tolist()

# Evaluation metrics
test_user_items = test_ratings.groupby('userId')['movieId'].apply(list).to_dict()

def precision_recall_at_k(user_id, k=5):
    if user_id not in test_user_items:
        return 0,0
    true_items = set(test_user_items[user_id])
    user_vector = svd_user_factors[user_id]
    scores = movie_embeddings[:, -n_components:].dot(user_vector)
    top_idx = np.argsort(-scores)[:k]
    recommended_items = set([idx_to_movie_id[i] for i in top_idx])
    hits = len(true_items & recommended_items)
    precision = hits / k
    recall = hits / len(true_items)
    return precision, recall

precisions, recalls = [], []
for uid in test_user_items.keys():
    p,r = precision_recall_at_k(uid, k=5)
    precisions.append(p)
    recalls.append(r)
print("Precision@5:", np.mean(precisions))
print("Recall@5:", np.mean(recalls))

def average_precision(user_id, k=5):
    if user_id not in test_user_items:
        return 0
    true_items = set(test_user_items[user_id])
    user_vector = svd_user_factors[user_id]
    scores = movie_embeddings[:, -n_components:].dot(user_vector)
    top_idx = np.argsort(-scores)[:k]
    hits = 0
    sum_prec = 0
    for i, idx in enumerate(top_idx):
        if idx_to_movie_id[idx] in true_items:
            hits += 1
            sum_prec += hits / (i+1)
    return sum_prec / min(len(true_items), k)

map_scores = [average_precision(uid, k=5) for uid in test_user_items.keys()]
print("MAP@5:", np.mean(map_scores))

# Usage examples
print("Recommendations for user 1:", recommend_for_user(1, 10))

!pip install sentence-transformers --quiet

import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import normalize
from sklearn.neighbors import NearestNeighbors

# Load datasets
movies = pd.read_csv('movies.csv')
try:
    tags = pd.read_csv('tags.csv')
except:
    tags = pd.DataFrame(columns=['userId','movieId','tag','timestamp'])

# Prepare movie metadata
movies['title'] = movies['title'].astype(str)
movies['genres'] = movies['genres'].fillna('').astype(str)
tags = tags.dropna(subset=['tag'])
tags['tag'] = tags['tag'].astype(str)
tag_agg = tags.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).reset_index()
movies = movies.merge(tag_agg, how='left', on='movieId')
movies['tag'] = movies['tag'].fillna('')
movies['metadata'] = movies['title'] + ' ' + movies['genres'].str.replace('|',' ') + ' ' + movies['tag']

# Optional: add plot summaries if available
# movies['metadata'] += ' ' + movies['plot']  # if you have a 'plot' column

# Generate semantic embeddings
model = SentenceTransformer('all-MiniLM-L6-v2')
movie_embeddings = model.encode(movies['metadata'].tolist(), convert_to_numpy=True, normalize_embeddings=True)

# Nearest Neighbors model
nn_model = NearestNeighbors(n_neighbors=10, metric='cosine', algorithm='brute')
nn_model.fit(movie_embeddings)

# Helper function to find title index
title_to_indices = {t.lower(): idx for idx, t in enumerate(movies['title'].values)}

def _find_title_index(query):
    q = query.strip().lower()
    if q in title_to_indices:
        return title_to_indices[q]
    candidates = movies[movies['title'].str.lower().str.contains(q, na=False)]
    if not candidates.empty:
        return int(candidates.index[0])
    raise KeyError(f"Movie '{query}' not found.")

# Context-based recommendations
def get_similar_movies(title, n=5):
    idx = _find_title_index(title)
    emb = movie_embeddings[idx].reshape(1, -1)
    dists, inds = nn_model.kneighbors(emb, n_neighbors=n+1)
    inds = [i for i in inds.flatten() if i != idx][:n]
    return movies['title'].iloc[inds].tolist()

# Example usage
print("Similar to 'Toy Story':", get_similar_movies("Toy Story", 5))

get_similar_movies("Forrest Gump", 10)